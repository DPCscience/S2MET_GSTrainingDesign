---
title: "Gather Environmental Variables"
output: html_notebook
---

## Introduction

This notebook will walk through the process of gathering and cleaning data that were serve as environmental covariables in the S2MET project

```{r setup}

library(stringr)
library(lubridate)
library(rnoaa)
library(geosphere)
library(soilDB)
library(foreign)
library(raster)
library(tidyverse)

# Directory
proj_dir <- "C:/Users/Jeff/Google Drive/Barley Lab/Projects/S2MET/"

env_var_dir <- file.path(proj_dir, "Environmental_Variables/")

# Figure dir
figure_dir <- "C:/Users/Jeff/Google Drive/Barley Lab/Projects/S2MET/Figures/"

# Trial Information
trial_info <- read_csv(file.path(env_var_dir, "trial_metadata.csv"))

```


## Data Collection

### NOAA Weather Data

Weather data will be collected by querying the NOAA API using the latitude/longitude coordinates from the trial metadata. These coordinates will be used to locate the nearest weather station and weather data will be drawn from these stations.

Data will be collected for two approaches:

1. Long-term (20 years of data will be collected)
2. Short term (only data within the year of the environment will be used)

Here is some code for downloading the GHCND stations and saving those station as an `.RData` file

```{r save.stations, eval=FALSE}

# # Load the list of cooperative stations - ONLY RUN ONCE -
# station_list <- ghcnd_stations()
# 
# save_file <- file.path(env_var_dir, "NOAA_Weather_Stations/GHCND_stations.RData")
# save("station_list", file = save_file)

```


Questions

1. Should data be collected only during the time in which the plants are grown? Values outside of this range could be set to NA and then imputed using the mean. In PCA this should not impact the separation between environments

2.


Pull out data from the past 15 years


```{r pull.data.noaa}

# API token
token <- 'ubmUkwrtxZOQLeyXZdtCMjkKcCTpPDcK'

# Start and end year
end_year <- ifelse(max(trial_info$Year) == 2017, 2016, max(trial_info$Year))
start_year <- end_year - 10

# Collect data from March 1 to August 31 of each year
start_date <- "03-01"
end_date <- "08-31"

# Set a threshold for missing data on a station-datatype basis
max_missing <- 0.20


# Desired data types
desired_vars <- c("TMIN", "TMAX", "PRCP")

# Load the list of cooperative stations
station_file <- file.path(env_var_dir, "Climate_Data/NOAA_Weather_Stations/GHCND_stations.RData")
load(station_file)


# Remove stations with no id and keep stations with the USC prefix
station_list1 <- station_list %>% 
  filter(str_detect(id, pattern = "USC|CA"),
         first_year <= start_year, 
         last_year >= end_year) %>%
  # Filter stations on whether all of the desired variables are present
  group_by(id) %>% 
  filter(all(desired_vars %in% element))

# Grab the unique stations
stations_unique <- station_list1 %>%
  distinct(.keep_all = TRUE) %>%
  ungroup()

# Create a data.frame of unique trial locations
locations_unique <- trial_info %>%
  filter(!is.na(Latitude), !is.na(Longitude)) %>%
  distinct(Location, .keep_all = T) %>% 
  select(Location, Latitude, Longitude)
  
# Create a list of distances from each trial location to all of the stations
# The distance is the great circle distance given the WGS84 ellipsoid
trial_loc_dist <- locations_unique %>%
  by_row(function(loc) {
    
    # Extract coordiates
    coord <- c(loc$Longitude, loc$Latitude)
    
    stations_unique %>% 
      group_by(id) %>% 
      summarize(dist_to_site = distGeo(p1 = coord, p2 = c(unique(longitude), unique(latitude))))
    
  }, .to = "station_dist")


# Find the 3 stations that are closest to each location
closest_station <- trial_loc_dist %>% 
  by_row(function(loc) top_n(loc$station_dist[[1]], n = -3, wt = dist_to_site), .collate = "rows") %>% 
  select(-station_dist, -.row)

# Group by environment and pull out data
trial_station_data <- closest_station %>%
  # Filter by the closest station. If tied, take the first
  group_by(Location) %>% 
  top_n(n = -1, wt = dist_to_site) %>% 
  dplyr::slice(1) %>% 
  # Apply function by rows
  by_row(function(loc_station) {
    
    # Change the station name
    station_id <- str_c("GHCND:", loc_station$id)
    
    # Create a list of length number of years
    data_list <- list()
    
    # Iterate over the requested years
    for (yr in seq(start_year, end_year)) {
    
      # Format the date
      start_ymd <- ymd(str_c(yr, start_date))
      end_ymd <- ymd(str_c(yr, end_date))
      
      # Pull data
      ncdc_data <- ncdc(datasetid = 'GHCND', datatypeid = desired_vars, stationid = station_id, 
                        startdate = as.character(start_ymd), enddate = as.character(end_ymd), 
                        limit = 1000, token = token)
      
      # Reformat and determine the number of days since the start date
      data_list[[as.character(yr)]] <- ncdc_data$data %>%
        as_data_frame() %>% 
        mutate(day = as.numeric(ymd(ymd_hms(date)) - start_ymd + 1),
               year = yr) %>%
        select(-date, -fl_t)
      
    } # Close the loop
    
    # Return the list
    bind_rows(data_list)
    
  }, .to = "data" )


# Quality-control the data
trial_station_data_qc <- trial_station_data %>% 
  group_by(Location, id) %>% 
  do(.$data[[1]]) %>%
  filter(fl_q == "") %>%
  # Reorder
  select(Location, id, year, day, datatype, value) %>%
  rename(location = Location)


# Complete the data.frame for all possible observations
trial_station_data_complete <- trial_station_data_qc %>%
  mutate(day = as.factor(str_pad(day, 3, pad = "0")),
         datatype = as.factor(datatype),
         year = as.factor(year)) %>% 
  complete(datatype, year, day) %>%
  unite(datatype_day, datatype, day, sep = ":", remove = FALSE) %>%
  # Convert days and year back to numeric
  mutate_at(vars(day, year), parse_number)

```



Deal with missing data by first assessing the level of missingness. If the level of missing data is above the threshold provided above, reject the data from that environment. Then look for data from the next closest station for each environment and extract data.

```{r clean.missing}

## Find the level of missing data for each environment
trial_station_missing <- trial_station_data_complete %>%
  group_by(location, datatype, year) %>%
  summarize(prop_miss = mean(is.na(value)))


## Pull out environments that need re-gathering - total
redo_env_all <- trial_station_missing %>% 
  filter(any(mean(prop_miss) > max_missing)) %>% 
  ungroup() %>% 
  distinct(location)

## Pull out environments that need re-gathering for 2015, 2016, or 2017
redo_env_rel <- trial_station_missing %>% 
  filter(year >= 2015) %>%
  filter(any(prop_miss > max_missing)) %>% 
  ungroup() %>% 
  distinct(location)

# Common environments
redo_env <- union(redo_env_all, redo_env_rel) %>% 
  pull(location)

# If the number of redo environments is greater than 0, proceed
if (nrow(redo_env) >= 1) {
  
  # Go back to the closest_station data frame and pull out the next closest station
  trial_redo_station_data <- closest_station %>% 
    filter(Location %in% redo_env) %>%
    group_by(Location) %>%
    # Take the bottom two, then the top 1
    top_n(n = -2, wt = dist_to_site) %>% 
    top_n(n = 1, wt = dist_to_site) %>%
    # If tied, select the second
    top_n(n = 1, wt = id) %>%
    by_row(function(loc_station) {
      
  
      # Change the station name
      station_id <- str_c("GHCND:", loc_station$id)
      
      # Create a list of length number of years
      data_list <- list()
      
      # Iterate over the requested years
      for (yr in seq(start_year, end_year)) {
      
        # Format the date
        start_ymd <- ymd(str_c(yr, start_date))
        end_ymd <- ymd(str_c(yr, end_date))
        
        # Pull data
        ncdc_data <- ncdc(datasetid = 'GHCND', datatypeid = desired_vars, stationid = station_id, 
                          startdate = as.character(start_ymd), enddate = as.character(end_ymd), 
                          limit = 1000, token = token)
        
        if (nrow(ncdc_data$data) == 0)
          next
        
        # Reformat and determine the number of days since the start date
        data_list[[as.character(yr)]] <- ncdc_data$data %>%
          as_data_frame() %>% 
          mutate(day = as.numeric(ymd(ymd_hms(date)) - start_ymd + 1),
                 year = yr) %>%
          select(-date, -fl_t)
        
      } # Close the loop
      
      # Return the list
      bind_rows(data_list)
      
    }, .to = "data") 
  
    # Quality-control the data
  trial_redo_station_data_qc <- trial_redo_station_data %>% 
    group_by(Location, id) %>% 
    do(.$data[[1]]) %>%
    filter(fl_q == "") %>%
    # Reorder
    select(Location, id, year, day, datatype, value) %>%
    rename(location = Location)


  # Complete the data.frame for all possible observations
  trial_redo_station_data_complete <- trial_redo_station_data_qc %>%
    mutate(day = as.factor(str_pad(day, 3, pad = "0")),
           datatype = as.factor(datatype),
           year = as.factor(year)) %>% 
    complete(datatype, year, day) %>%
    unite(datatype_day, datatype, day, sep = ":", remove = FALSE) %>%
    # Convert days and year back to numeric
    mutate_at(vars(day, year), parse_number)
    
  
  # Quality-control the data
  trial_redo_station_data_qc <- trial_redo_station_data %>% 
    filter(fl_q == "")
  
  # Complete the data.frame for all possible observations
  trial_redo_station_data_complete <- trial_redo_station_data_qc %>%
    mutate(days = as.factor(str_pad(days, 3, pad = "0")),
           datatype = as.factor(datatype)) %>% 
    complete(datatype, days) %>%
    unite(datatype_days, datatype, days, sep = ":", remove = FALSE)
  
  ## Find the level of missing data for each environment
  trial_station_missing <- trial_redo_station_data_complete %>%
    group_by(location, datatype, year) %>%
    summarize(prop_miss = mean(is.na(value)))
  
  
  ## Pull out environments that need re-gathering - total
  redo_env_all <- trial_station_missing %>% 
    filter(any(mean(prop_miss) > max_missing)) %>% 
    ungroup() %>% 
    distinct(location)
  
  ## Pull out environments that need re-gathering for 2015, 2016, or 2017
  redo_env_rel <- trial_station_missing %>% 
    filter(year >= 2015) %>%
    filter(any(prop_miss > max_missing)) %>% 
    ungroup() %>% 
    distinct(location)
  
  # Combine the redo dataset with the remaining data
  trial_station_data_complete1 <- trial_station_data_complete %>% 
    filter(!location %in% redo_env) %>%
    list(., trial_redo_station_data_complete) %>%
    bind_rows() %>%
    arrange(location)
  
}


```


Assemble relevant data to save


```{r save.data}

# Gather station data
noaa_stations_used <- stations_unique %>%
  filter(id %in% str_replace(trial_station_data_complete$id, "GHCND:", "")) %>%
  select(-element)

# rename the weather data
noaa_trial_data_complete <- trial_station_data_complete1

save_file <- file.path(env_var_dir, "Climate_Data/NOAA_Data/noaa_stations_trial_data.RData")
save("noaa_stations_used", "noaa_trial_data_complete", file = save_file)


```


### Soil Data

Soil data for sites in the U.S. will be queried from the NRCS Soil Survey, and soil data for sites in Canada will be queried from the National Soil Database

First filter the `trial_info` data.frame for U.S. versus Canadian locations

```{r split.trials}

# Define the grid size around a latitude/longitude coordinate
grid_size <- 1e-6

# Filter out the Canadian sites and group by location
canadian <- c("EON", "CPE", "PQC")

trial_info_us <- trial_info %>%
    filter(!str_detect(Environment, str_c(canadian, collapse = "|")))

trial_info_can <- trial_info %>%
    filter(str_detect(Environment, str_c(canadian, collapse = "|")))

```

This database returns variables per horizon per component (these can be found in a description [here](https://sdmdataaccess.nrcs.usda.gov/documents/TableColumnDescriptionsReport.pdf)). The following variables will be kept:


  Variable      | Definition
----------------|---------------------------------------------------------------
hzdept_r        | Distance from top of soil to top of horizon (cm)
hzdepb_r        | Distance from top of soil to bottom of horizon (cm)
sandtotal_r     | Mineral particles 0.05mm to 2.0mm in equivalent diameter as a weight percentage of the less than 2 mm fraction.
silttotal_r     | Mineral particles 0.002 to 0.05mm in equivalent diameter as a weight percentage of the less than 2.0mm fraction.
claytotal_r     | Mineral particles less than 0.002mm in equivalent diameter as a weight percentage of the less than 2.0mm fraction.
om_r            | The amount by weight of organic matter expressed as a weight percentage of the less than 2 mm soil material
ph1to1h2o_r     | pH




```{r soil.nrcs}

# Iterate over the locations
trial_info_us_mukey <- trial_info_us %>%
  # Filter NA
  filter(!is.na(Planting_date)) %>%
  distinct(Environment, .keep_all = TRUE) %>% 
  split(.$Environment) %>%
  purrr::map(function(loc_info) {
    
    # Create the coordinate grid
    b <- c(loc_info$Longitude - grid_size,
           loc_info$Latitude - grid_size,
           loc_info$Longitude + grid_size,
           loc_info$Latitude + grid_size)
    
    # Find information based on the coordinates and grid size
    mapunit <- mapunit_geom_by_ll_bbox(b)
    
    # Return the mukey
    loc_info %>%
      mutate(mukey = unique(mapunit$mukey)) }) %>%
  # Bind rows
  bind_rows()

# For each location and using the mukey, gather relevant soil variables
trial_info_us_vars <- trial_info_us_mukey %>%
  group_by(Environment) %>%
  do({
    
    # Create the query string to get the cokey
    query <- str_c("SELECT * FROM component WHERE mukey = '", .$mukey, "'")
    
    # Query and get the cokey of the most abundant component
    cokey <- SDA_query(query) %>%
      top_n(n = 1, wt = comppct_r) %>%
      dplyr::select(cokey, comppct_r)
    
    query <- str_c("SELECT * FROM chorizon WHERE cokey = '", cokey$cokey, "'")
    # Use the cokey to get characteristic data
    SDA_query(query) %>%
      gather(variable, value, -hzname, -hzdept_r, -hzdepb_r) %>%
      mutate(comppct_r = cokey$comppct_r) })

# Subset the variables
# Also parse the values of the variables to numbers
desired_vars <- c("hzdept_r", "hzdepb_r", "sandtotal_r", "silttotal_r", "claytotal_r", "om_r", "ph1to1h2o_r")

trial_info_us_vars_sub <- trial_info_us_vars %>%
  filter(variable %in% desired_vars) %>%
  mutate(value = parse_number(value))

# What is the level of missing data per variable?
trial_info_us_vars_sub %>%
  group_by(variable) %>%
  summarize(prop_miss = mean(is.na(value)))

# Per location?
trial_info_us_vars_sub %>%
  group_by(Environment) %>%
  summarize(prop_miss = mean(is.na(value)))

# Merge data with the remaining trial info
nrcs_trial_data_complete <- full_join(trial_info, trial_info_us_vars_sub, "Environment")

# Save the data
save_file <- file.path(env_var_dir, "Soil_Data/USDA_NRCS/nrcs_trial_soil_data.RData")
save("nrcs_trial_data_complete", file = save_file)

```


Query the National Soil Database for site information in Canada

The analagous variables from the USDA Soil Survey in the National Soil DataBase (Canada) are:

  Variable      | NSDB Analogue
----------------|---------------------------------------------------------------
hzdept_r        | UDEPTH
hzdepb_r        | LDEPTH - UDEPTH
sandtotal_r     | TSAND
silttotal_r     | TSILT
claytotal_r     | TCLAY
om_r            | ORGCARB
ph1to1h2o_r     | PH2




```{r soil.nsdb}

# Set the directory for pulling the Canadian soil data
nsdb_dir <- file.path(env_var_dir, "Soil_Data/NSDB/")

# Designate the shapefile with polygon information
shape_file <- file.path(nsdb_dir, "ca_all_slc_v3r2")
# Designate the dbf with soil component information
comp_file <- file.path(nsdb_dir, "ca_all_slc_v3r2_cmp.dbf")
# Designate the dbf file with soil layer information
layer_file <- file.path(nsdb_dir, "soil_layer_canada_v2r20170602.dbf")

# Read in files
# nsdb_poly <- readOGR(dsn = nsdb_dir, layer = "ca_all_slc_v3r2")
# Use raster instead
nsdb_poly <- shapefile(shape_file)

# Transform projection information
nsdb_poly <- spTransform(x = nsdb_poly, CRSobj = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")


# Read in the dbf files
nsdb_comp <- read.dbf(file = comp_file, as.is = TRUE)
nsdb_layer <- read.dbf(file = layer_file, as.is = TRUE)

## For each trial, find the soil information corresponding to the lat/long
trial_info_can_vars <- trial_info_can %>%
  filter(!is.na(Planting_date)) %>%
  group_by(Environment) %>%
  do({
    
    # Create the box
    bbx <- c(.$Longitude - 1e-6,
             .$Longitude + 1e-6,
             .$Latitude - 1e-6,
             .$Latitude + 1e-6)

    # Crop the polygon
    nsdb_poly_crop <- crop(x = nsdb_poly, y = extent(bbx))
  
    # Get the POLY_ID
    poly_id <- nsdb_poly_crop$POLY_ID

    # Subset the components file for that POLY_ID and take the soil component that is most abudant
    comp_select <- nsdb_comp %>%
      filter(POLY_ID == poly_id) %>%
      top_n(n = 1, wt = PERCENT)
    
    # Now use the SOIL_ID from this selction to obtain information about that soil using the
    # nsdb_layer file
    soil_id <- as.character(comp_select$SOIL_ID)
    
    soil_select <- nsdb_layer %>% 
      filter(SOIL_ID == soil_id)
    
    # Combine the soil data back to the component data
    full_join(comp_select, soil_select, by = c("SOIL_ID", "PROVINCE", "SOIL_CODE", "MODIFIER", "PROFILE")) %>%
      gather(variable, value, -HZN_MAS, -UDEPTH, -LDEPTH) %>%
      select(HZN_MAS, UDEPTH, LDEPTH, variable, value) })

# Vector of desired variables
desired_vars <- c("TSAND", "TSILT", "TCLAY", "ORGCARB", "PH2")

# Subset the data for the desired variables
trial_info_can_vars_sub <- trial_info_can_vars %>%
  filter(variable %in% desired_vars) %>%
  mutate(value = parse_number(value),
         value = ifelse(value == -9, NA, value))

# What is the level of missing data per variable?
trial_info_can_vars_sub %>%
  group_by(variable) %>%
  summarize(prop_miss = mean(is.na(value)))

# Per location?
trial_info_can_vars_sub %>%
  group_by(Environment) %>%
  summarize(prop_miss = mean(is.na(value)))

# Merge data with the remaining trial info
nsdb_trial_data_complete <- full_join(trial_info, trial_info_can_vars_sub, "Environment")

# Save the data
save_file <- file.path(env_var_dir, "Soil_Data/NSDB/nsdb_trial_soil_data.RData")
save("nsdb_trial_data_complete", file = save_file)


```



Combine the soil data and create common variable names

```{r soils.comb}

nrcs_file <- file.path(env_var_dir, "Soil_Data/USDA_NRCS/nrcs_trial_soil_data.RData")
nsdb_file <- file.path(env_var_dir, "Soil_Data/NSDB/nsdb_trial_soil_data.RData")

load(nrcs_file)
load(nsdb_file)

# Expand each df
nrcs_expand <- nrcs_trial_data_complete %>%
  filter(!is.na(variable)) %>%
  select(-comppct_r) %>%
  spread(variable, value)

nsdb_expand <- nsdb_trial_data_complete %>%
  filter(!is.na(variable)) %>%
  spread(variable, value)

# Combine
soil_data_complete <- full_join(nrcs_expand, nsdb_expand, by = c("Environment", "Location", "Year", "Latitude", "Longitude", "Planting_date", "hzname" = "HZN_MAS", "hzdept_r"= "UDEPTH", "hzdepb_r" = "LDEPTH", "claytotal_r" = "TCLAY", "om_r" = "ORGCARB", "ph1to1h2o_r" = "PH2", "sandtotal_r" = "TSAND", "silttotal_r" = "TSILT"))

# Re-tidy
soil_data_complete <- soil_data_complete %>%
  gather(variable, value, -Environment:-hzdepb_r)

# Export
save_file <- file.path(env_var_dir, "Soil_Data/complete_trial_soil_data.RData")
save("soil_data_complete", file = save_file)

```



### Transformation and Compilation 


```{r transform}

# Load both climate and soil datasets
noaa_file <- file.path(env_var_dir, "Climate_Data/NOAA_Data/noaa_stations_trial_data.RData")
load(noaa_file)

soil_file <- file.path(env_var_dir, "Soil_Data/complete_trial_soil_data.RData")
load(soil_file)

## Soil data
# Edit the soil data (define topsoil and subsoil, then scale and center). Note in cm
depth_cutoff <- 40

soil_data_complete_scale <- soil_data_complete %>%
  filter(!is.na(Planting_date)) %>%
  mutate(layer = ifelse(hzdepb_r <= depth_cutoff, "topsoil", "subsoil")) %>%
  unite(variable, variable, layer, sep = "_") %>%
  group_by(Environment, Location, variable) %>%
  summarize(value = mean(value, na.rm = TRUE)) %>%
  # Center and scale
  group_by(variable) %>% 
  mutate(value = scale(value)) %>% 
  rename(environment = Environment, location = Location) %>%
  ungroup()



## Climate Data
# Center and scale by datatype and day
noaa_trial_data_complete_scale <- noaa_trial_data_complete %>% 
  group_by(datatype_day) %>% 
  mutate(value = scale(value)) %>%
  ungroup()

# Create two datasets for each environment - the year itself and the 10-year average
noaa_trial_data_oneyear <- trial_info %>%
  rename_all(str_to_lower) %>%
  filter(year != 2017) %>% # EDIT THIS LATER
  by_row(function(env) {
    
    # Year itself
    weather <- noaa_trial_data_complete_scale %>% 
      filter(year == env$year, location == env$location) %>%
      select(id, datatype_day, value) %>%
      rename(variable = datatype_day)
    
    # Soil data
    soil <- soil_data_complete_scale %>% 
      filter(environment == env$environment, location == env$location) %>%
      select(variable, value)
    
    bind_rows(weather, soil)
    
  }, .to = "one_year_df")



# For each data_type - day combination, take the 10-year average
average_year <- 10

noaa_trial_data_average <- trial_info %>%
  rename_all(str_to_lower) %>%
  filter(year != 2017) %>% # EDIT THIS LATER
  by_row(function(env) {
    
    # Long-term average
    weather <- noaa_trial_data_complete_scale %>% 
      filter(between(year, env$year - (average_year - 1), env$year), location == env$location) %>% 
      group_by(id, datatype_day) %>% 
      summarize(value = mean(value, na.rm = T)) %>%
      rename(variable = datatype_day)
    
    # Soil data
    soil <- soil_data_complete_scale %>% 
      filter(environment == env$environment, location == env$location) %>%
      select(variable, value)
    
    bind_rows(weather, soil)
    
  }, .to = "long_term_df")

## Combine all data
environmental_data <- full_join(noaa_trial_data_oneyear, noaa_trial_data_average) %>%
  mutate_at(vars(environment, location, year), as.factor)

# Create matrices
one_year_mat <- environmental_data %>% 
  select(environment, one_year_df) %>% 
  unnest(one_year_df) %>%
  select(-id) %>%
  # Impute missing with mean
  group_by(variable) %>%
  mutate(value = ifelse(is.na(value), mean(value, na.rm = T), value)) %>%
  ungroup() %>%
  spread(variable, value) %>%
  data.frame(row.names = .$environment, stringsAsFactors = FALSE) %>%
  select(-environment) %>%
  as.matrix()

multi_year_mat <- environmental_data %>% 
  select(environment, long_term_df) %>% 
  unnest(long_term_df) %>%
  select(-id) %>%
  # Impute missing with mean
  group_by(variable) %>%
  mutate(value = ifelse(is.na(value), mean(value, na.rm = T), value)) %>%
  ungroup() %>%
  spread(variable, value) %>%
  data.frame(row.names = .$environment, stringsAsFactors = FALSE) %>%
  select(-environment) %>%
  as.matrix()

# Save the data
save_file <- file.path(env_var_dir, "environmental_data_compiled.RData")
save("environmental_data", "one_year_mat", "multi_year_mat", file = save_file)


```


### PCA

Assess the relationship between environment using PCA

```{r env.PCA}

# Load data
load_file <- file.path(env_var_dir, "environmental_data_compiled.RData")
load(load_file)

## Try some PCA
# One-year
one_year_pca <- svd(one_year_mat)
# Lambda
lambda <- one_year_pca$d %>% 
  {. / sum(.)}

one_year_pca_tidy <- one_year_pca$u %>% 
  data.frame(environment = row.names(one_year_mat), .) %>%
  rename_at(vars(starts_with("X")), str_replace, pattern = "X", replacement = "PC")

g1 <- full_join(environmental_data, one_year_pca_tidy) %>% 
  ggplot(aes(x = PC1, y = PC2, col = year)) + 
  geom_point() +
  geom_text(aes(label = environment), nudge_x = -0.05, nudge_y = -0.01) +
  ylab(paste("PC2 (", lambda[2], ")", sep = "")) +
  xlab(paste("PC1 (", lambda[1], ")", sep = "")) +
  labs( title = "PCA of Year-Specific Environmental Variables")

# Save image
save_file <- file.path(figure_dir, "one_year_env_data_pca.jpg")
ggsave(filename = save_file, plot = g1, height = 5, width = 7)


# Multi-year
multi_year_pca <- svd(multi_year_mat)
# Lambda
lambda <- multi_year_pca$d %>% 
  {. / sum(.)}

multi_year_pca_tidy <- multi_year_pca$u %>% 
  data.frame(environment = row.names(multi_year_mat), .) %>%
  rename_at(vars(starts_with("X")), str_replace, pattern = "X", replacement = "PC")

g2 <- full_join(environmental_data, multi_year_pca_tidy) %>% 
  ggplot(aes(x = PC1, y = PC2, col = year)) + 
  geom_point() +
  geom_text(aes(label = environment), nudge_x = -0.05, nudge_y = -0.01) +
  ylab(paste("PC2 (", lambda[2], ")", sep = "")) +
  xlab(paste("PC1 (", lambda[1], ")", sep = "")) +
  labs( title = "PCA of 10-Year Average Environmental Variables")

save_file <- file.path(figure_dir, "multi_year_env_data_pca.jpg")
ggsave(filename = save_file, plot = g2, height = 5, width = 7)

```







## Create Regressors

### PCA

```{r}

# PCA
station_pca <- svd(station_data_mat_imp)

plot(station_pca$u[,1], station_pca$u[,2])
text(station_pca$u[,1], station_pca$u[,2], labels = row.names(station_data_mat_imp))

# Create an environmental relationship matrix
E_cor <- cor(station_pca$u)
E_cov <- cov(station_pca$u)
dimnames(E) <- list(rownames(station_data_mat_imp),rownames(station_data_mat_imp))

# Save this data
save_file <- file.path(env_var_dir, "env_cov_mat.RData")
save("E", file = save_file)


# CLuster
E_dist <- dist(E)
E_clust <- hclust(E_dist)

plot(E_clust)


```






### Supplemental


Try using the world harmoized soil database

This is from a technical note found (here)[http://www.css.cornell.edu/faculty/dgr2/teach/R/R_HWSD.pdf].

Also using the `RODBC` package, with documentation in the form of a vignette

```{r hwsd}

library(raster)
library(RODBC)


hwsd <- raster("HWSD/HWSD_RASTER/hwsd.bil")

# Add projection information
proj4string(hwsd) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"

# Iterate over locations
trial_global_ID <- trial_info %>%
  filter(!is.na(Latitude)) %>%
  mutate(global_ID = NA)

for (i in seq(nrow(trial_global_ID))) {
  
  # Create a coordinate box using the latitude/longitude
  b <- c(trial_global_ID[i,]$Longitude - 1e-6,
         trial_global_ID[i,]$Longitude + 1e-6,
         trial_global_ID[i,]$Latitude - 1e-6,
         trial_global_ID[i,]$Latitude + 1e-6)
    
  # Crop the raster file
  hwsd_crop <- crop(x = hwsd, y = extent(b))
  
  trial_global_ID[i,]$global_ID <- unique(hwsd_crop)
  
}

# Connect to the mdb data_base
con <- odbcConnectAccess2007(access.file = "HWSD/HWSD_new.accdb")

# Extract the HWSD_DATA table
hwsd_data <- sqlFetch(channel = temp, sqtable = "HWSD_DATA")

# For each location, subset the hwsd data based on the unique ID
trial_info_hwsd <- trial_global_ID %>% 
  group_by_(.dots = names(.)) %>%
  do(hwsd = filter(hwsd_data, MU_GLOBAL == .$global_ID))


```


Combine all of the soil layer information from each Canadian province into a single data.frame

```{r combine.prov}

prov_layer_files <- list.files(file.path(nsdb_dir, "Province_Layers"), full.names = TRUE)

# Read in all data
prov_layer <- lapply(X = prov_layer_files, read.dbf, as.is = TRUE) %>%
  bind_rows()


save_file <- file.path(nsdb_dir, "soil_layer_canada_v2r20170602.dbf")
# Save the data
write.dbf(dataframe = prov_layer, file = save_file)




```



